concat_channels_dim: 1000
dmel_groups: 10
codec_ckpt_dir: /home/wzy/projects/dmel_codec/dmel_codec/ckpt/dmel_codec

data:
  stage: fit
  train_cuts_path: /home/wzy/projects/dmel_codec/train_cuts_windows-3_min_duration-3.0_max_duration-None.jsonl.gz
  val_cuts_path: /home/wzy/projects/dmel_codec/val_cuts_sample-128.jsonl.gz

  train_max_durations: 210.0 # dynamic batch size, seconds
  train_num_workers: 40
  val_max_durations: 4 # dynamic batch size, seconds
  val_num_workers: 1

  world_size: 6 # same with GPU nums

model:
  accumulate_grad: 1 # gradient accumulation steps

  encoder:
    input_channels: 10
    residual_channels: 70
    residual_layers: 20

  quantizer:
    _target_: dmel_codec.models.modules.dowmsample_fsq.DownsampleFiniteScalarQuantize
    levels:
    - 7
    - 5
    - 5
    is_dmel: true
    downsample_factor:
    - 2
    - 2

  vocoder:
    ckpt_path: /home/wzy/projects/bigvgan_v2_24khz_100band_256x/bigvgan_generator.pt
    h_path: /home/wzy/projects/bigvgan_v2_24khz_100band_256x/config.json

  optimizer:
    lr: 1e-5

  lr_scheduler:
    lr_lambda:
      num_training_steps: 1000000 # 1000k
      final_lr_ratio: 0.01

callbacks:
  model_checkpoint:
    dirpath: ${codec_ckpt_dir}
    filename: '{epoch:03d}-{step:06d}_20hz'

tensorboard_logger:
  _target_: lightning.pytorch.loggers.TensorBoardLogger
  save_dir: /home/wzy/projects/dmel_codec/tb_logs
  name: dmel_codec_20hz
  log_graph: true

trainer:
  accelerator: gpu
  precision: 32
  devices: -1
  # devices: [0]
  max_steps: 10000000 # 10000k / 2 = 5000k steps
  val_check_interval: 8000